---
title: UKHSA - Exercise
author: "Ofer Engel"
date: '`r Sys.Date()`'
execute: 
  warning: false
  message: false
format: 
   html:
     theme: flatly
     df-print: paged
     toc-location: right
     toc: true
     code-fold: true
     code-summary: "Show the code"
editor_options: 
  chunk_output_type: console
---


```{r}
#| label: setup
#| warning: false
#| echo: true

rm(list = ls())
library(here)
source(here("bin", "lib.R"))

# load data here
df <- read_csv(
  'data/G7_Summer_2025_dataset.csv', 
  show_col_types = FALSE
  )  |> 
  clean_names() |> 
  mutate(
    age = case_when(
      age < 0 ~ NA, 
      age > 98 ~ NA, 
      .default = age
    ), 
    age = cut_interval(age, length = 10)
  ) |> 
    mutate(
      height = case_when(
      height <= 0 ~ NA, 
      height > 200 ~ NA, 
      .default = height
      )
    ) |> 
    select(-id, -blood_type) |> 
    mutate(
      death_from_heatstroke = ifelse(death_from_heatstroke, "yes", "no"), 
      death_from_heatstroke = fct_relevel(death_from_heatstroke, "yes")
      )


df |> 
  tabyl(death_from_heatstroke)

# baseline brier (lower is better!)
pi_emp <- 0.7931
pi_emp * (1 - pi_emp)


```


## Some exploratory data analysis (EDA)

Let's look at some distributions and correlations first

```{r}
#| warning: false
#| echo: true
skim(df)


```


Now let's look at some predictors


```{r}
#| label: predictors
#| warning: false

df |> 
  recipe(death_from_heatstroke ~ ., data = _) |> 
  step_other(all_nominal_predictors(), threshold = .05) |> 
  step_unknown(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> 
  step_dummy(death_from_heatstroke, one_hot = FALSE) |> 
  step_nzv() |> 
  prep() |> 
  bake(df) |>  
  get_cors("death_from_heatstroke_no") |> 
  drop_na(estimate) 

```



## Modelling

We will use just an xgboost model

```{r}
#| label: data splitting and recipe
#| 
set.seed(123)
df_split <- df |> 
  initial_split(strata = death_from_heatstroke)
df_train <- training(df_split)
df_test <- testing(df_split)
df_metrics <- metric_set(accuracy, roc_auc, sens, spec)

set.seed(234)
df_folds <- vfold_cv(df, v = 5, strata = death_from_heatstroke)


rec <-   df |> 
  recipe(death_from_heatstroke ~ ., data = _) |> 
  step_unknown(all_nominal_predictors()) |> 
  step_other(all_nominal_predictors(), threshold = .1) |> 
  step_novel() |> 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> 
  step_nzv() 


# rec |> 
#    prep() |> 
#    bake(new_data = NULL) |> 
#    View()

# rec |> 
#   prep() |> 
#   tidy(number = 1)


```

### My XGBOOST model

Now let's build a tunable xgboost model specification, tuning a lot of the important model hyperparameters, and combine it with our feature engineering recipe in a workflow(). We can also create a custom xgb_grid to specify what parameters I want to try out, like not-too-small learning rate, avoiding tree stubs, etc. I chose this parameter grid to get reasonable performance in a reasonable amount of tuning time.

```{r}

xgb_spec <-
  boost_tree(
    trees = tune(),
    min_n = tune(),
    mtry = tune(),
    learn_rate = 0.1
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

xgb_wf <- workflow(rec, xgb_spec)

```

Now we can tune across the grid of parameters and our resamples. Since we are trying quite a lot of hyperparameter combinations, let’s use racing to quit early on clearly bad hyperparameter combinations. We won't actually run this though, only once. 


```{r}
#| eval: false

library(finetune)
library(future)
plan(multisession)
set.seed(345)
xgb_rs <- tune_race_anova(
  xgb_wf,
  resamples = df_folds,
  grid = 15,
  metrics = df_metrics,
  control = control_race(verbose_elim = TRUE)
)
xgb_rs

# Watch the race
plot_race(xgb_rs)
xgb_rs |> 
  collect_metrics() |> 
  select(-.config)

show_best(
  xgb_rs, 
  metric = "roc_auc"
  ) |> 
    select(-.config)
# # A tibble: 4 × 9
#    mtry trees min_n .metric .estimator  mean     n std_err .config
#   <int> <int> <int> <chr>   <chr>      <dbl> <int>   <dbl> <chr>  
# 1     1   714    23 roc_auc binary     0.807     5 0.00248 Prepro…
# 2    31   143    12 roc_auc binary     0.799     5 0.00437 Prepro…
# 3    11   429    37 roc_auc binary     0.796     5 0.00294 Prepro…



select_best(xgb_rs, metric = "accuracy") |> 
  dput()

xgb_res <- (
  best_xgb_wf <- xgb_wf %>%
    finalize_workflow(
      select_best(xgb_rs, metric = "accuracy") 
      )
    ) %>%
  fit_resamples(
    df_folds, 
    control = control_resamples(save_pred = TRUE),
    metrics = metric_set(roc_auc, accuracy, sens, spec, brier_class)
    ) 

# The very best tuned model
# # A tibble: 1 × 5
#    mtry trees min_n learn_rate .config              
#   <int> <int> <int>      <dbl> <chr>                
# 1    11   584     8     0.0110 Preprocessor1_Model24

```


Now for the evaluation. We use `fit_resamples()` to fit the resamples with the numerically optimal result from the tuning set of models.

```{r}

best_xgb_wf <- xgb_wf |> 
  update_model(
      boost_tree(
      trees = 714L,
      min_n = 23L,
      mtry = 1L,
      learn_rate = 0.0110069417125221
    ) %>%
    set_engine("xgboost") %>%
    set_mode("classification")
  )

xgb_res <- best_xgb_wf |> 
  fit_resamples(
    df_folds, 
    control = control_resamples(save_pred = TRUE),
    metrics = metric_set(roc_auc, accuracy, sens, spec, brier_class)
    ) 

xgb_res |> 
  collect_metrics() |> 
  select(-.config)

# A tibble: 5 × 5
#   .metric     .estimator  mean     n  std_err
#   <chr>       <chr>      <dbl> <int>    <dbl>
# 1 accuracy    binary     0.863     5 0.00173 
# 2 brier_class binary     0.113     5 0.00116 
# 3 roc_auc     binary     0.799     5 0.00359 
# 4 sens        binary     0.369     5 0.00705 
# 5 spec        binary     0.992     5 0.000643



xgb_res |> 
  collect_predictions() |> 
  select(-.config, -id) |> 
  conf_mat(truth = death_from_heatstroke, .pred_class) |> 
  autoplot()

xgb_res |> 
  collect_predictions() |> 
  select(-.config, -id) |> 
  roc_curve(truth = death_from_heatstroke, .pred_yes) |> 
  autoplot()


```


## Fit last

Now we train one last time with the train data and fit the test data using `last_fit()` to fit one final time to the training data and evaluate one final time on the testing data.


```{r}
#| label: last fit

# Fit Last
xgb_last <-
  best_xgb_wf |> 
  last_fit(
    df_split, 
    metrics = metric_set(roc_auc, accuracy, sens, spec, brier_class)
    )

xgb_last |> 
  collect_metrics() |> 
  select(-.config)
# # A tibble: 5 × 3
#   .metric     .estimator .estimate
#   <chr>       <chr>          <dbl>
# 1 accuracy    binary         0.864
# 2 sens        binary         0.376
# 3 spec        binary         0.992
# 4 roc_auc     binary         0.789
# 5 brier_class binary         0.112

# xgb_last |> 
#   collect_predictions() |> 
#   tabyl(death_from_heatstroke, .pred_class) |> 
#   adorn_percentages() |> 
#   adorn_pct_formatting()

xgb_last |> 
  collect_predictions() %>%
  conf_mat(truth = death_from_heatstroke, .pred_class) |> 
  autoplot()


collect_predictions(xgb_last) %>%
  roc_curve(truth = death_from_heatstroke, .pred_yes) |> 
  autoplot()

```


### Feature importance

Checking for feature importance

```{r}

library(vip)
extract_workflow(xgb_last) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point", num_features = 15) + 
  theme(axis.text.y = element_text(size = 12))
```